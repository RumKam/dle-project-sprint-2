# Нейросеть для автодополнения текстов

Репозиторий содержит учебный проект по созданию нейросетевой модели автодополнения текстов. Задача решается в двух вариантах: лёгкая модель на базе LSTM и более тяжёлая предобученная трансформерная модель `distilgpt2`. Результаты сравниваются, и на основе анализа даются рекомендации по использованию в мобильном приложении.

## Описание задачи
Автодополнение текста – это задача генерации продолжения по заданному началу фразы. В проекте рассматриваются два подхода:
1. **Модель на основе LSTM** – обучается с нуля на предоставленном датасете коротких текстов.
2. **Предобученная трансформерная модель `distilgpt2`** – используется без дообучения (inference) для генерации продолжений.

Обе модели должны уметь генерировать текст пошагово (токен за токеном) до достижения токена окончания последовательности или ограничения длины.

## Бизнес-контекст
Проект имитирует реальную задачу из соцсетевого приложения. Необходимо добавить функцию автодополнения текстовых постов. Из-за ограничений мобильных устройств (память, скорость) важно оценить, можно ли обойтись лёгкой моделью (LSTM) или же качество тяжёлой модели (трансформера) настолько выше, что стоит адаптировать её под мобильную платформу.

## Цели проекта
1. Подготовить и очистить датасет коротких текстов (sentiment140).
2. Реализовать и обучить LSTM-модель для автодополнения.
3. Оценить качество LSTM на тестовых данных (метрики ROUGE, примеры генераций).
4. Запустить предобученную модель `distilgpt2` для тех же входных текстов и замерить её качество.
5. Сравнить результаты и решить, какую модель выбрать для внедрения.

## Этапы проекта
Проект выполняется поэтапно, код для каждого этапа находится в соответствующих модулях папки `src/`.

### Этап 0. Подготовка окружения
- Установка зависимостей (см. `requirements.txt`).
- Инициализация репозитория.

### Этап 1. Сбор и подготовка данных
- Загрузка датасета [Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140) (или его локальной копии).
- Очистка текстов: приведение к нижнему регистру, удаление ссылок, упоминаний, лишних символов, токенизация.
- Формирование обучающих пар `X` → `Y` (предсказание следующего токена).
- Разделение на train / val / test в пропорции 80/10/10.
- Создание `torch.Dataset` и `DataLoader`.

### Этап 2. Реализация рекуррентной сети
- Написание класса LSTM-модели с методом `forward` для предсказания следующего токена.
- Реализация метода генерации (`generate`) для пошагового дополнения текста.

### Этап 3. Тренировка модели
- Реализация цикла обучения с подсчётом функции потерь (кросс-энтропия).
- Реализация метрики ROUGE-1 и ROUGE-2 на валидационной выборке.
- Подбор гиперпараметров (размер батча, размер скрытого слоя, количество эпох).
- Сохранение весов обученной модели.
- Вывод примеров автодополнений.

### Этап 4. Использование предобученного трансформера
- Загрузка модели `distilgpt2` через `transformers.pipeline`.
- Генерация продолжений для тестовых текстов (первые 3/4 текста как промпт).
- Замер ROUGE-1 и ROUGE-2 для предсказанных продолжений.
- Подбор параметров генерации (`max_length`, `top_k`, `temperature` и т.д.).

### Этап 5. Формулирование выводов
- Сравнение метрик двух моделей.
- Анализ примеров генераций (качество, связность, соответствие контексту).
- Оценка времени работы и потребления памяти (приблизительно).
- Рекомендации для интеграции в мобильное приложение.

## Метрики качества
Для оценки качества генерации используются метрики **ROUGE-1** и **ROUGE-2**, которые измеряют совпадение униграмм и биграмм между сгенерированным продолжением и реальным окончанием текста. Так как тексты короткие, большие n-граммы не учитываются.

Дополнительно оценивается качество вручную на нескольких примерах.

## Структура репозитория
```
dle-project-sprint-2/
├── data/                             # Датасеты
│   ├── tweets.txt                     # "Сырой" скачанный датасет
│   ├── dataset_processed.csv          # Очищенный датасет
│   ├── train.csv                      # Тренировочная выборка
│   ├── val.csv                        # Валидационная выборка
│   └── test.csv                       # Тестовая выборка
│
├── src/                              # Исходный код
│   ├── data_utils.py                  # Функции для загрузки и очистки данных
│   ├── next_token_dataset.py           # torch.Dataset для обучения
│   ├── lstm_model.py                   # LSTM модель
│   ├── lstm_train.py                    # Обучение LSTM
│   ├── eval_lstm.py                     # Оценка LSTM (ROUGE, примеры)
│   ├── eval_transformer_pipeline.py     # Оценка distilgpt2
│
├── configs/                          # Конфигурационные файлы (YAML)
│
├── models/                           # Сохранённые веса моделей
│
├── solution.ipynb                    # Ноутбук с итоговым решением (вызовы функций, визуализация, выводы)
├── requirements.txt                  # Зависимости проекта
└── README.md                         # Документация проекта
```

## Установка и запуск
1. Клонируйте репозиторий:
   ```bash
   git clone https://github.com/RumKam/dle-project-sprint-2.git
   cd dle-project-sprint-2
   ```

2. Установите зависимости:
   ```bash
   pip install -r requirements.txt
   ```

3. Скачайте датасет [Sentiment140](https://www.kaggle.com/datasets/kazanova/sentiment140) и поместите его в папку `data/tweets.txt`.

4. Запустите предобработку данных:
   ```bash
   python src/data_utils.py
   ```
   Или выполните соответствующие ячейки в `solution.ipynb`.

5. Для обучения LSTM выполните:
   ```bash
   python src/lstm_train.py
   ```

6. Для оценки моделей используйте скрипты `eval_lstm.py` и `eval_transformer_pipeline.py`, либо ноутбук `solution.ipynb`.

## Результаты

### LSTM модель
- **ROUGE-1 на валидации:** 0.0895  
- **ROUGE-2 на валидации:** 0.0114  
- **Пример генерации:**  
  *Вход:* "thanks for following nice to meet"
  *Продолжение:* "you.."  

### Трансформер (distilgpt2)
- **ROUGE-1 на валидации:** 0.0684  
- **ROUGE-2 на валидации:** 0.0084  
- **Пример генерации:**  
  *Вход:* "can you say exhausted? man, i think i only got 1-2 hours of sleep last nightthis morning. but relay for"  
  *Продолжение:* "you and i will be there for you all"


## Выводы
- **Модель, показавшая лучшее качество**  
  Трансформер дает более связные тексты, но более низкие метрики.
- **Рекомендации:**  
  - Если ограничения памяти/скорости критичны, можно использовать LSTM (возможно, с дообучением или доработками архитектуры).  
  - Если допустимо небольшое увеличение требований к ресурсам ради качества, стоит рассмотреть `distilgpt2` или его квантизированную версию.

## Используемые технологии
- Python 3.10+
- PyTorch
- Transformers (Hugging Face)
- Datasets, Tokenizers
- ROUGE (`rouge-score`)
- NumPy, Pandas

---
